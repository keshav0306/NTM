{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from ntm_data import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NTM(nn.Module):\n",
    "    def __init__(self, hidden_state_dim, input_dim, num_memory_cells, memory_cell_dim):\n",
    "        super(NTM, self).__init__()\n",
    "    \n",
    "        self.num_memory_cells = num_memory_cells\n",
    "        self.memory_cell_dim = memory_cell_dim\n",
    "\n",
    "        # Declaring only 1 read head and only 1 write head for now\n",
    "        self.read_head = ReadHead(hidden_state_dim, memory_cell_dim, num_memory_cells)\n",
    "        self.write_head = WriteHead(hidden_state_dim, memory_cell_dim, num_memory_cells)\n",
    "        self.read_weights_init = torch.rand(num_memory_cells)\n",
    "        self.write_weights_init = torch.rand(num_memory_cells)\n",
    "        \n",
    "        self.h_init = torch.nn.Parameter(torch.randn(hidden_state_dim))\n",
    "        # RNN internal layers that computes the affine transform which maps the input and the hidden state\n",
    "        # to the next state\n",
    "\n",
    "        # GRU unit\n",
    "        self.rnn_wx_update = torch.nn.Linear(input_dim, hidden_state_dim)\n",
    "        self.rnn_wh_update = torch.nn.Linear(hidden_state_dim, hidden_state_dim)\n",
    "\n",
    "        self.rnn_wx_reset = torch.nn.Linear(input_dim, hidden_state_dim)\n",
    "        self.rnn_wh_reset = torch.nn.Linear(hidden_state_dim, hidden_state_dim)\n",
    "\n",
    "        self.rnn_wx_hidden = torch.nn.Linear(input_dim, hidden_state_dim)\n",
    "        self.rnn_wh_hidden = torch.nn.Linear(hidden_state_dim, hidden_state_dim)\n",
    "\n",
    "        # Secondary state change based on read vector\n",
    "        self.rnn_read_secondary_effect = torch.nn.Linear(memory_cell_dim, hidden_state_dim)\n",
    "        self.rnn_self_secondary_effect = torch.nn.Linear(hidden_state_dim, hidden_state_dim)\n",
    "\n",
    "        # Define the ouptut layer depending on the use case\n",
    "        self.output_affine = torch.nn.Linear(hidden_state_dim, input_dim)\n",
    "\n",
    "    def forward(self, input, output):\n",
    "        # Assuming the input and output to be of the shape (N, T, W)\n",
    "        # N -> Mini batch size, T -> The size of the temporal component, W -> The size of one such component\n",
    "\n",
    "        N, T, W = input.shape\n",
    "        prev_h = torch.tile(self.h_init, (N, 1))\n",
    "        # Intialize the memory to zeros\n",
    "        memory = torch.zeros(N, self.num_memory_cells, self.memory_cell_dim)\n",
    "        prev_weights_read = torch.tile(self.read_weights_init, (N, 1))\n",
    "        prev_weights_write = torch.tile(self.write_weights_init, (N, 1))\n",
    "        loss = 0\n",
    "        for i in range(T):\n",
    "            z = torch.sigmoid(self.rnn_wx_update(input[:,i,:]) + self.rnn_wh_update(prev_h))\n",
    "            r = torch.sigmoid(self.rnn_wx_reset(input[:,i,:]) + self.rnn_wh_reset(prev_h))\n",
    "            prev_h = (1 - z) * prev_h + z * torch.tanh(self.rnn_wx_hidden(input[:,i,:]) + self.rnn_wh_hidden(prev_h) * r)\n",
    "            # Pass the hidden state to the read head first, then the write head\n",
    "            read, prev_weights_read = self.read_head(memory, prev_weights_read, prev_h)\n",
    "            memory, prev_weights_write = self.write_head(memory, prev_weights_write, prev_h)\n",
    "            # Use read to update the hidden state of the controller\n",
    "            secondary_read_effect = self.rnn_read_secondary_effect(read)\n",
    "            secondary_self_effect = self.rnn_self_secondary_effect(prev_h)\n",
    "            prev_h = torch.tanh(secondary_read_effect + secondary_self_effect)\n",
    "            # Based on this prev_h, calculate the output and subsequently the loss\n",
    "            out = self.output_affine(prev_h)\n",
    "            loss += torch.sum(((out - output[:,i,:]) ** 2))\n",
    "\n",
    "        for i in range(T):\n",
    "            z = torch.sigmoid(self.rnn_wh_update(prev_h))\n",
    "            r = torch.sigmoid(self.rnn_wh_reset(prev_h))\n",
    "            prev_h = (1 - z) * prev_h + z * torch.tanh(self.rnn_wh_hidden(prev_h) * r)\n",
    "            # Pass the hidden state to the read head first, then the write head\n",
    "            read, prev_weights_read = self.read_head(memory, prev_weights_read, prev_h)\n",
    "            memory, prev_weights_write = self.write_head(memory, prev_weights_write, prev_h)\n",
    "            # Use read to update the hidden state of the controller\n",
    "            secondary_read_effect = self.rnn_read_secondary_effect(read)\n",
    "            secondary_self_effect = self.rnn_self_secondary_effect(prev_h)\n",
    "            prev_h = torch.tanh(secondary_read_effect + secondary_self_effect)\n",
    "            # Based on this prev_h, calculate the output and subsequently the loss\n",
    "            out = self.output_affine(prev_h)\n",
    "            loss += torch.sum(((out - output[:,i + T,:]) ** 2))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test(self, input):\n",
    "        N, T, W = input.shape\n",
    "        prev_h = torch.tile(self.h_init, (N, 1))\n",
    "        # Intialize the memory to zeros\n",
    "        memory = torch.zeros(N, self.num_memory_cells, self.memory_cell_dim)\n",
    "        prev_weights_read = torch.tile(self.read_weights_init, (N, 1))\n",
    "        prev_weights_write = torch.tile(self.write_weights_init, (N, 1))\n",
    "        output = []\n",
    "        for i in range(T):\n",
    "            z = torch.sigmoid(self.rnn_wx_update(input[:,i,:]) + self.rnn_wh_update(prev_h))\n",
    "            r = torch.sigmoid(self.rnn_wx_reset(input[:,i,:]) + self.rnn_wh_reset(prev_h))\n",
    "            prev_h = (1 - z) * prev_h + z * torch.tanh(self.rnn_wx_hidden(input[:,i,:]) + self.rnn_wh_hidden(prev_h) * r)\n",
    "            # Pass the hidden state to the read head first, then the write head\n",
    "            read, prev_weights_read = self.read_head(memory, prev_weights_read, prev_h)\n",
    "            memory, prev_weights_write = self.write_head(memory, prev_weights_write, prev_h)\n",
    "            # Use read to update the hidden state of the controller\n",
    "            secondary_read_effect = self.rnn_read_secondary_effect(read)\n",
    "            secondary_self_effect = self.rnn_self_secondary_effect(prev_h)\n",
    "            prev_h = torch.tanh(secondary_read_effect + secondary_self_effect)\n",
    "            # Based on this prev_h, calculate the output and subsequently the loss\n",
    "            out = self.output_affine(prev_h)\n",
    "            output.append(out)\n",
    "\n",
    "        for i in range(T):\n",
    "            z = torch.sigmoid(self.rnn_wh_update(prev_h))\n",
    "            r = torch.sigmoid(self.rnn_wh_reset(prev_h))\n",
    "            prev_h = (1 - z) * prev_h + z * torch.tanh(self.rnn_wh_hidden(prev_h) * r)\n",
    "            # Pass the hidden state to the read head first, then the write head\n",
    "            read, prev_weights_read = self.read_head(memory, prev_weights_read, prev_h)\n",
    "            memory, prev_weights_write = self.write_head(memory, prev_weights_write, prev_h)\n",
    "            # Use read to update the hidden state of the controller\n",
    "            secondary_read_effect = self.rnn_read_secondary_effect(read)\n",
    "            secondary_self_effect = self.rnn_self_secondary_effect(prev_h)\n",
    "            prev_h = torch.tanh(secondary_read_effect + secondary_self_effect)\n",
    "            # Based on this prev_h, calculate the output and subsequently the loss\n",
    "            out = self.output_affine(prev_h)\n",
    "            output.append(out)\n",
    "\n",
    "        return output\n",
    "\n",
    "class ReadHead(nn.Module):\n",
    "    def __init__(self, input_dim, memory_cell_dim, num_memory_cells):\n",
    "        super(ReadHead, self).__init__()\n",
    "\n",
    "        # Based on the hidden_dim of the state of the controller, computes 5 components\n",
    "\n",
    "        # 1). k_t, the key vector which is a vector of dim same as that of any memory cell (memory_cell_dim)\n",
    "        # 2). beta_t which is a scalar used to denote key strength for content based addressing\n",
    "        # 3). g_t which is a scalar, which is used to selectively choose between the w_{t-1} and content based weights\n",
    "        # 4). s_t which is the convolution shift vector used to give the probablity of each shift, of dimension num_memory_cells\n",
    "        # 5). gamma_t which is a scalar used to define the precision of the final weights (used for removing blurriness)\n",
    "\n",
    "        # The computation for the final weights depends upon the memory cells and the previous weights\n",
    "        # Assume prev_weights are given by the caller in the forward function\n",
    "\n",
    "        # Defining 5 hidden layers of appropriate dimensions\n",
    "\n",
    "        self.k_affine = nn.Linear(input_dim, memory_cell_dim)\n",
    "        self.beta_affine = nn.Linear(input_dim, 1)\n",
    "        self.g_affine = nn.Linear(input_dim, 1)\n",
    "        self.s_affine = nn.Linear(input_dim, num_memory_cells)\n",
    "        self.gamma_affine = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, memory, prev_weights, controller_hidden_state):\n",
    "        \n",
    "        # Assume memory of shape (N, num_memory_cells, memory_cell_dim)\n",
    "        # prev_weights of shape (N, num_memory_cells)\n",
    "        # controller_hidden_state of shape (N, H)\n",
    "        # This function will be called at each time step (0 ... T-1)\n",
    "        # returns the read vector and the new weights\n",
    "\n",
    "        # Use relu for different non-linearities for now\n",
    "        N, H = prev_weights.shape\n",
    "        N, num_memory_cells, memory_cell_dim = memory.shape\n",
    "\n",
    "        key = torch.relu(self.k_affine(controller_hidden_state)) # (N, memory_cell_dim)\n",
    "\n",
    "        # Compute the cosine simmilarity between the key and all the memory cells\n",
    "        # Cosine similarity is just the dot product between the 2 vectors\n",
    "        cos_similarity = torch.zeros(N, num_memory_cells)\n",
    "        for i in range(N):\n",
    "            cos_similarity = key @ torch.transpose(memory[i,:,:], 0, 1)\n",
    "\n",
    "        # Compute beta and multiply each component of the cos_similarity matrix by beta\n",
    "        beta = self.beta_affine(controller_hidden_state)\n",
    "        cos_similarity = cos_similarity * beta\n",
    "\n",
    "        # wc is given by the softmax along dim = -1\n",
    "        w_content = torch.softmax(cos_similarity, dim = -1)\n",
    "\n",
    "        # calculate the g scalar\n",
    "        g = self.g_affine(controller_hidden_state)\n",
    "\n",
    "        # Interpolate among the prev_weights and the w_content based on the value of g\n",
    "        w_g = g * w_content + (1 - g) * prev_weights\n",
    "\n",
    "        # Compute the shift vector and apply softmax to it\n",
    "        shift = torch.softmax(self.s_affine(controller_hidden_state), dim = -1)\n",
    "\n",
    "        # Compute the circular convolution\n",
    "        circ = torch.zeros((N, num_memory_cells))\n",
    "        shift_other_way = torch.flip(shift, [1])\n",
    "        cat = torch.cat([shift_other_way, shift_other_way], dim = 1)\n",
    "        for i in range(num_memory_cells):\n",
    "            circ[:,i] = torch.sum(cat[:,num_memory_cells - 1 - i:2*num_memory_cells - 1 - i] * w_g, dim = -1)\n",
    "        \n",
    "        # Calculate gamma and raise weach weight to gamma and normalize\n",
    "        gamma = self.g_affine(controller_hidden_state)\n",
    "        w_g = circ ** gamma\n",
    "        w_next = w_g / torch.sum(w_g, dim = -1).unsqueeze(-1)\n",
    "\n",
    "        # Read from the memory, now that we have calculated the weights\n",
    "        read = torch.zeros((N, memory_cell_dim))\n",
    "        for i in range(N):\n",
    "            read[i,:] = w_next[i,:] @ memory[i,:,:]\n",
    "\n",
    "        return read, w_next\n",
    "    \n",
    "class WriteHead(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, memory_cell_dim, num_memory_cells):\n",
    "        super(WriteHead, self).__init__()\n",
    "\n",
    "        # Based on the hidden_dim of the state of the controller, computes 5 components\n",
    "\n",
    "        # 1). k_t, the key vector which is a vector of dim same as that of any memory cell (memory_cell_dim)\n",
    "        # 2). beta_t which is a scalar used to denote key strength for content based addressing\n",
    "        # 3). g_t which is a scalar, which is used to selectively choose between the w_{t-1} and content based weights\n",
    "        # 4). s_t which is the convolution shift vector used to give the probablity of each shift, of dimension num_memory_cells\n",
    "        # 5). gamma_t which is a scalar used to define the precision of the final weights (used for removing blurriness)\n",
    "        # 6). The erase vector which is a vector of dimension memory_cell_dim\n",
    "        # 7). The add vector which is a vector of dimension memory_cell_dim\n",
    "\n",
    "        # The computation for the final weights depends upon the memory cells and the previous weights\n",
    "        # Assume prev_weights are given by the caller in the forward function\n",
    "\n",
    "        # Defining 7 hidden layers of appropriate dimensions\n",
    "\n",
    "        self.k_affine = nn.Linear(input_dim, memory_cell_dim)\n",
    "        self.beta_affine = nn.Linear(input_dim, 1)\n",
    "        self.g_affine = nn.Linear(input_dim, 1)\n",
    "        self.s_affine = nn.Linear(input_dim, num_memory_cells)\n",
    "        self.gamma_affine = nn.Linear(input_dim, 1)\n",
    "\n",
    "        self.erase_affine = nn.Linear(input_dim, memory_cell_dim)\n",
    "        self.add_affine = nn.Linear(input_dim, memory_cell_dim)\n",
    "\n",
    "    def forward(self, memory, prev_weights, controller_hidden_state):\n",
    "        \n",
    "        # Assume memory of shape (N, num_memory_cells, memory_cell_dim)\n",
    "        # prev_weights of shape (N, num_memory_cells)\n",
    "        # controller_hidden_state of shape (N, H)\n",
    "        # This function will be called at each time step (0 ... T-1)\n",
    "        # returns the read vector and the new weights\n",
    "        # Use relu for different non-linearities for now\n",
    "        N, H = prev_weights.shape\n",
    "        N, num_memory_cells, memory_cell_dim = memory.shape\n",
    "\n",
    "        key = torch.relu(self.k_affine(controller_hidden_state)) # (N, memory_cell_dim)\n",
    "\n",
    "        # Compute the cosine simmilarity between the key and all the memory cells\n",
    "        # Cosine similarity is just the dot product between the 2 vectors\n",
    "        cos_similarity = torch.zeros(N, num_memory_cells)\n",
    "        for i in range(N):\n",
    "            cos_similarity = key @ torch.transpose(memory[i,:,:], 0, 1)\n",
    "\n",
    "        # Compute beta and multiply each component of the cos_similarity matrix by beta\n",
    "        beta = self.beta_affine(controller_hidden_state)\n",
    "        cos_similarity = cos_similarity * beta\n",
    "\n",
    "        # wc is given by the softmax along dim = -1\n",
    "        w_content = torch.softmax(cos_similarity, dim = -1)\n",
    "\n",
    "        # calculate the g scalar\n",
    "        g = self.g_affine(controller_hidden_state)\n",
    "\n",
    "        # Interpolate among the prev_weights and the w_content based on the value of g\n",
    "        w_g = g * w_content + (1 - g) * prev_weights\n",
    "\n",
    "        # Compute the shift vector and apply softmax to it\n",
    "        shift = torch.softmax(self.s_affine(controller_hidden_state), dim = -1)\n",
    "\n",
    "        # Compute the circular convolution\n",
    "        circ = torch.zeros((N, num_memory_cells))\n",
    "        shift_other_way = torch.flip(shift, [1])\n",
    "        cat = torch.cat([shift_other_way, shift_other_way], dim = 1)\n",
    "        for i in range(num_memory_cells):\n",
    "            circ[:,i] = torch.sum(cat[:,num_memory_cells - 1 - i:2*num_memory_cells - 1 - i] * w_g, dim = -1)\n",
    "        \n",
    "        # Calculate gamma and raise each weight to gamma and normalize\n",
    "        gamma = self.g_affine(controller_hidden_state)\n",
    "        w_g = circ ** gamma\n",
    "        w_next = w_g / torch.sum(w_g, dim = -1).unsqueeze(-1)\n",
    "\n",
    "        # Change memory inplace\n",
    "        # Calculate erase and add vectors\n",
    "        erase = self.erase_affine(controller_hidden_state)\n",
    "        add = self.add_affine(controller_hidden_state)\n",
    "        write = torch.zeros((N, memory_cell_dim))\n",
    "        new_memory = torch.zeros_like(memory)\n",
    "        for i in range(N):\n",
    "            to_erase = torch.ones(num_memory_cells, memory_cell_dim) - torch.reshape(w_next[i, :], (num_memory_cells, 1)) * torch.tile(erase[i], (num_memory_cells, 1))\n",
    "            write = memory[i,:,:] * to_erase\n",
    "            to_add = torch.reshape(w_next[i, :], (num_memory_cells, 1)) * torch.tile(add[i], (num_memory_cells, 1))\n",
    "            new_memory[i,:,:] = write + to_add\n",
    "\n",
    "        return new_memory, w_next\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Common Parameters For Both Encoder and Decoder\n",
    "# TODO make the parameters different for both of them, define separately\n",
    "\n",
    "hidden_state_dim = 128\n",
    "input_dim = 8\n",
    "num_memory_cells = 128\n",
    "memory_cell_dim = 20\n",
    "batch_size = 50\n",
    "seq_len = 10\n",
    "\n",
    "input, output = copy_task_data(batch_size, seq_len, input_dim)\n",
    "\n",
    "# Create a NTM instance\n",
    "ntm = NTM(hidden_state_dim, input_dim, num_memory_cells, memory_cell_dim)\n",
    "\n",
    "epochs = 1000\n",
    "learning_rate = 0.001\n",
    "\n",
    "optimizer = torch.optim.Adam(ntm.parameters(), learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "\n",
    "    loss = ntm(input, output)\n",
    "    print(loss)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # for p in ntm.parameters():\n",
    "        # print(p.grad)\n",
    "    torch.nn.utils.clip_grad_norm_(ntm.parameters(), 10.0)\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
