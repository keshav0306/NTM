{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "from rnn_layers_torch import *\n",
    "from load_data import *\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, word_to_idx, wordvec_dim, hidden_dim, cell_type, seed, device):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        vocab_size = len(word_to_idx)\n",
    "        self.start_token = word_to_idx[\"<START>\"]\n",
    "        self.null_token = word_to_idx[\"<NULL>\"]\n",
    "        self.end_token = word_to_idx[\"<END>\"]\n",
    "        self.cell_type = cell_type\n",
    "        self.params = {}\n",
    "\n",
    "        if(seed is not None):\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.params[\"W_embed\"] = np.random.randn(vocab_size, wordvec_dim)\n",
    "        self.params[\"W_embed\"] /= 100\n",
    "\n",
    "        dim_mul = {\"lstm\": 4, \"rnn\": 1}[cell_type]\n",
    "        self.params[\"Wx\"] = np.random.randn(wordvec_dim, dim_mul * hidden_dim)\n",
    "        self.params[\"Wx\"] /= np.sqrt(wordvec_dim)\n",
    "        self.params[\"Wh\"] = np.random.randn(hidden_dim, dim_mul * hidden_dim)\n",
    "        self.params[\"Wh\"] /= np.sqrt(hidden_dim)\n",
    "        self.params[\"b\"] = np.zeros(dim_mul * hidden_dim)\n",
    "\n",
    "        self.params[\"W_vocab\"] = np.random.randn(hidden_dim, vocab_size)\n",
    "        self.params[\"W_vocab\"] /= np.sqrt(hidden_dim)\n",
    "        self.params[\"b_vocab\"] = np.zeros(vocab_size)\n",
    "\n",
    "        for key in self.params.keys():\n",
    "            self.params[key] = self.params[key].astype(np.float32)\n",
    "            self.params[key] = torch.from_numpy(self.params[key])\n",
    "            self.params[key] = self.params[key].to(device)\n",
    "            self.params[key].requires_grad = True\n",
    "\n",
    "\n",
    "    def forward(self, captions, h_init):\n",
    "        '''returns all the hidden states of the RNN as a tensor of shape (N, T, H)\n",
    "        '''\n",
    "\n",
    "        captions_in = captions[:,:-1]\n",
    "        N = captions.shape[0]\n",
    "        # h0 = torch.tile(h_init, (N, 1))\n",
    "        h0 = h_init\n",
    "        h = None\n",
    "\n",
    "        # Generate word embeddings from captions\n",
    "        inputs = word_embedding_forward(captions_in, self.params[\"W_embed\"])\n",
    "\n",
    "        # RNN forward pass\n",
    "        if(self.cell_type == \"rnn\"):\n",
    "            h = rnn_forward(inputs, h0, self.params[\"Wx\"], self.params[\"Wh\"], self.params[\"b\"])\n",
    "        elif(self.cell_type == \"lstm\"):\n",
    "            h = lstm_forward(inputs, h0, self.params[\"Wx\"], self.params[\"Wh\"], self.params[\"b\"])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def load(self, parameters):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key] = parameters[key]\n",
    "            self.params[key].requires_grad = True\n",
    "    \n",
    "    \n",
    "class RNNAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, word_to_idx, wordvec_dim, hidden_dim, cell_type, seed, device):\n",
    "        super(RNNAttention, self).__init__()\n",
    "\n",
    "        vocab_size = len(word_to_idx)\n",
    "        self.start_token = word_to_idx[\"<START>\"]\n",
    "        self.null_token = word_to_idx[\"<NULL>\"]\n",
    "        self.end_token = word_to_idx[\"<END>\"]\n",
    "        self.cell_type = cell_type\n",
    "        self.params = {}\n",
    "\n",
    "        if(seed is not None):\n",
    "            np.random.seed(seed)\n",
    "\n",
    "        self.params[\"W_embed\"] = np.random.randn(vocab_size, wordvec_dim)\n",
    "        self.params[\"W_embed\"] /= 100\n",
    "\n",
    "        dim_mul = {\"lstm\": 4, \"rnn\": 1}[cell_type]\n",
    "        self.params[\"Wx\"] = np.random.randn(wordvec_dim, dim_mul * hidden_dim) * 0.01\n",
    "        self.params[\"Wx\"] /= np.sqrt(wordvec_dim)\n",
    "        self.params[\"Wh\"] = np.random.randn(hidden_dim, dim_mul * hidden_dim) * 0.01\n",
    "        self.params[\"Wh\"] /= np.sqrt(hidden_dim)\n",
    "        self.params[\"b\"] = np.zeros(dim_mul * hidden_dim)\n",
    "\n",
    "        self.params[\"W_vocab\"] = np.random.randn(hidden_dim, vocab_size)\n",
    "        self.params[\"W_vocab\"] /= np.sqrt(hidden_dim)\n",
    "        self.params[\"b_vocab\"] = np.zeros(vocab_size)\n",
    "\n",
    "        self.params[\"W_attention1\"] = np.random.randn(hidden_dim * 2, hidden_dim) * 0.001\n",
    "        self.params[\"W_attention1\"] /= np.sqrt(hidden_dim * 2)\n",
    "        self.params[\"b_attention1\"] = np.zeros(hidden_dim)\n",
    "        self.params[\"W_attention2\"] = np.zeros((hidden_dim, 1))\n",
    "        self.params[\"W_context\"] = np.random.randn(hidden_dim, dim_mul * hidden_dim)\n",
    "\n",
    "        for key in self.params.keys():\n",
    "            self.params[key] = self.params[key].astype(np.float32)\n",
    "            self.params[key] = torch.from_numpy(self.params[key])\n",
    "            self.params[key] = self.params[key].to(device)\n",
    "            self.params[key].requires_grad = True\n",
    "\n",
    "\n",
    "    def forward(self, hidden_enc_states, captions, h_init, mask):\n",
    "        '''returns all the hidden states of the RNN as a tensor of shape (N, T, H)\n",
    "        '''\n",
    "\n",
    "        captions_in = captions[:,:-1]\n",
    "        N = captions.shape[0]\n",
    "        # h0 = torch.tile(h_init, (N, 1))\n",
    "        h0 = h_init\n",
    "        h = None\n",
    "\n",
    "        # Generate word embeddings from captions\n",
    "        inputs = word_embedding_forward(captions_in, self.params[\"W_embed\"])\n",
    "\n",
    "        # RNN forward pass\n",
    "        if(self.cell_type == \"rnn\"):\n",
    "            h = rnn_attention_forward(inputs, h0, self.params[\"Wx\"], self.params[\"Wh\"], self.params[\"b\"], \\\n",
    "                                      hidden_enc_states, self.params[\"W_attention1\"],\\\n",
    "                                         self.params[\"b_attention1\"], self.params[\"W_attention2\"], mask, self.params[\"W_context\"])\n",
    "        elif(self.cell_type == \"lstm\"):\n",
    "            h = lstm_attention_forward(inputs, h0, self.params[\"Wx\"], self.params[\"Wh\"], self.params[\"b\"], \\\n",
    "                                      hidden_enc_states, self.params[\"W_attention1\"],\\\n",
    "                                         self.params[\"b_attention1\"], self.params[\"W_attention2\"], mask, self.params[\"W_context\"])\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def load(self, parameters):\n",
    "        for key in self.params.keys():\n",
    "            self.params[key] = parameters[key]\n",
    "            self.params[key].requires_grad = True\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, word_to_idx, wordvec_dim, hidden_dim, cell_type, seed, device):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.RNN = RNN(word_to_idx, wordvec_dim, hidden_dim, cell_type, seed, device)\n",
    "        self.h_init = torch.randn(hidden_dim, requires_grad=True)\n",
    "    \n",
    "    def forward(self, captions):\n",
    "        ''' captions are of shape (N, T + 1)\n",
    "        '''\n",
    "        h0 = torch.tile(self.h_init, (captions.shape[0], 1))\n",
    "        hidden_states = self.RNN(captions, h0) # (N, T, H)\n",
    "        mask = captions[:,1:] != self.RNN.null_token\n",
    "        # Only returns the end hidden state (the state till the <END> token)\n",
    "        end = torch.where(captions[:,1:] == self.RNN.end_token)\n",
    "        return hidden_states, hidden_states[end], mask\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, word_to_idx, wordvec_dim, hidden_dim, cell_type, seed, device):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.RNN = RNNAttention(word_to_idx, wordvec_dim, hidden_dim, cell_type, seed, device)\n",
    "        # Attention layers for calculating the attention between hidden state of decoder\n",
    "        # and all the hidden states of the encoder.\n",
    "        # The 2nd dimension of attention_w can be arbitrary\n",
    "        \n",
    "    def forward(self, captions, h_init, hidden_enc_states, mask):\n",
    "        ''' captions are of shape (N, T + 1)\n",
    "        '''\n",
    "        hidden_states = self.RNN(hidden_enc_states, captions, h_init, mask) # (N, T, H)\n",
    "        # assume the sentence starts with a <START> token and ends with a <END> token\n",
    "        captions_out = captions[:,1:]\n",
    "\n",
    "        # don't consider the loss where the token is <NULL>\n",
    "        mask = captions_out != self.RNN.null_token\n",
    "\n",
    "        out = temporal_affine_forward(hidden_states, self.RNN.params[\"W_vocab\"], self.RNN.params[\"b_vocab\"])\n",
    "        loss = temporal_softmax_loss(out, captions_out, mask)\n",
    "        return loss\n",
    "\n",
    "class NMT(nn.Module):\n",
    "    def __init__(self, word_to_idx_enc, word_to_idx_dec, wordvec_dim, hidden_dim, cell_type, seed, device):\n",
    "        super(NMT, self).__init__()\n",
    "\n",
    "        # TODO -> allow for different hidden states dimensions for encoder and decoder\n",
    "\n",
    "        self.encoder = Encoder(word_to_idx_enc, wordvec_dim, hidden_dim, cell_type, seed, device)\n",
    "        self.decoder = Decoder(word_to_idx_dec, wordvec_dim, hidden_dim, cell_type, seed, device)\n",
    "        self.mid_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, captions1, captions2):\n",
    "        ''' Both captions1 and captions2 start with <START> and end with <END> and are of shape (N, T + 1) (T is different for both)\n",
    "        '''\n",
    "        hidden_states, h_encoder, mask = self.encoder(captions1) # (N, T, H) and (N, H) and (N, H)\n",
    "        h_init = self.mid_layer(h_encoder)\n",
    "        loss = self.decoder(captions2, h_init, hidden_states, mask)\n",
    "        return loss\n",
    "    \n",
    "    def load(self, weights):\n",
    "        self.encoder.RNN.load(weights[\"params_enc\"])\n",
    "        self.decoder.RNN.load(weights[\"params_dec\"])\n",
    "        self.mid_layer.weight = nn.Parameter(weights[\"mid_layer\"][0])\n",
    "        self.mid_layer.bias = nn.Parameter(weights[\"mid_layer\"][1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_en = \"english.txt\"\n",
    "file_es = \"spanish.txt\"\n",
    "lwflag = 0 # 0 for words, 1 for letter\n",
    "word_to_idx = None\n",
    "\n",
    "word_to_idx_en = make_dict(file_en)\n",
    "word_to_idx_es = make_dict(file_es)\n",
    "\n",
    "reverse_dict_en = {}\n",
    "reverse_dict_es = {}\n",
    "\n",
    "for keys, value in word_to_idx_en.items():\n",
    "    reverse_dict_en[value] = keys\n",
    "\n",
    "for keys, value in word_to_idx_es.items():\n",
    "    reverse_dict_es[value] = keys\n",
    "\n",
    "# All the tensors will be allocated by default on the device\n",
    "# Except some cases where we are using torch.from_numpy. Why?\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.set_default_device(device)\n",
    "\n",
    "# Common Parameters For Both Encoder and Decoder\n",
    "# TODO make the parameters different for both of them, define separately\n",
    "hidden_dim = 256\n",
    "word_vec_dim = 256\n",
    "seed = 4\n",
    "architecture = \"lstm\"\n",
    "epochs = 10000\n",
    "learning_rate = 0.001\n",
    "# Create a NTM instance\n",
    "nmt = NMT(word_to_idx_en, word_to_idx_es, word_vec_dim, hidden_dim, architecture, seed, device)\n",
    "\n",
    "# Uncomment below lines and comment line in the for loop for cheking overfitting\n",
    "# data_en, data_es = load_data_nmt(word_to_idx_en, word_to_idx_es, file_en, file_es, lines_count=1, max_train=30, hardcode=485)\n",
    "\n",
    "# for i in range(len(data_en)):\n",
    "#     words = [reverse_dict_en[val] for val in data_en[i]]\n",
    "#     for word in words:\n",
    "#         print(word + \" \", end = \"\")\n",
    "\n",
    "# print(\"\")\n",
    "\n",
    "# for i in range(len(data_es)):\n",
    "#     words = [reverse_dict_es[val] for val in data_es[i]]\n",
    "#     for word in words:\n",
    "#         print(word + \" \", end = \"\")\n",
    "\n",
    "# print(\"\")\n",
    "\n",
    "# data_en = torch.from_numpy(data_en)\n",
    "# data_es = torch.from_numpy(data_es)\n",
    "# data_en = data_en.to(device)\n",
    "# data_es = data_es.to(device)\n",
    "\n",
    "parameters = [nmt.mid_layer.weight, nmt.mid_layer.bias]\n",
    "\n",
    "for key in nmt.encoder.RNN.params.keys():\n",
    "    parameters.append(nmt.encoder.RNN.params[key])\n",
    "\n",
    "for key in nmt.decoder.RNN.params.keys():\n",
    "    parameters.append(nmt.decoder.RNN.params[key])\n",
    "\n",
    "optimizer = optim.Adam(parameters, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_file = \"check.pt\"\n",
    "loaded_weights = torch.load(weights_file, map_location=device)\n",
    "nmt.load(loaded_weights)\n",
    "optimizer.load_state_dict(loaded_weights[\"optime\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(194.0360, grad_fn=<DivBackward0>)\n",
      "tensor(142.7063, grad_fn=<DivBackward0>)\n",
      "tensor(128.5942, grad_fn=<DivBackward0>)\n",
      "tensor(120.7977, grad_fn=<DivBackward0>)\n",
      "tensor(177.4941, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/keshavgupta/Desktop/RNN_Attention/attention.ipynb Cell 5\u001b[0m in \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keshavgupta/Desktop/RNN_Attention/attention.ipynb#W3sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     param\u001b[39m.\u001b[39mretain_grad()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keshavgupta/Desktop/RNN_Attention/attention.ipynb#W3sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad() \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/keshavgupta/Desktop/RNN_Attention/attention.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keshavgupta/Desktop/RNN_Attention/attention.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(parameters, \u001b[39m1.0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keshavgupta/Desktop/RNN_Attention/attention.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# for param in parameters:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/keshavgupta/Desktop/RNN_Attention/attention.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     \u001b[39m# print(param.grad)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/_tensor.py:478\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"Computes the gradient of current tensor w.r.t. graph leaves.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \n\u001b[1;32m    433\u001b[0m \u001b[39mThe graph is differentiated using the chain rule. If the tensor is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[39m        used to compute the attr::tensors.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39;49mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39;49m,),\n\u001b[1;32m    481\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m    482\u001b[0m         gradient\u001b[39m=\u001b[39;49mgradient,\n\u001b[1;32m    483\u001b[0m         retain_graph\u001b[39m=\u001b[39;49mretain_graph,\n\u001b[1;32m    484\u001b[0m         create_graph\u001b[39m=\u001b[39;49mcreate_graph,\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[1;32m    487\u001b[0m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/overrides.py:1534\u001b[0m, in \u001b[0;36mhandle_torch_function\u001b[0;34m(public_api, relevant_args, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m \u001b[39mif\u001b[39;00m _is_torch_function_mode_enabled():\n\u001b[1;32m   1531\u001b[0m     \u001b[39m# if we're here, the mode must be set to a TorchFunctionStackMode\u001b[39;00m\n\u001b[1;32m   1532\u001b[0m     \u001b[39m# this unsets it and calls directly into TorchFunctionStackMode's torch function\u001b[39;00m\n\u001b[1;32m   1533\u001b[0m     \u001b[39mwith\u001b[39;00m _pop_mode_temporarily() \u001b[39mas\u001b[39;00m mode:\n\u001b[0;32m-> 1534\u001b[0m         result \u001b[39m=\u001b[39m mode\u001b[39m.\u001b[39;49m__torch_function__(public_api, types, args, kwargs)\n\u001b[1;32m   1535\u001b[0m     \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNotImplemented\u001b[39m:\n\u001b[1;32m   1536\u001b[0m         \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/utils/_device.py:62\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m func \u001b[39min\u001b[39;00m _device_constructors() \u001b[39mand\u001b[39;00m kwargs\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     61\u001b[0m     kwargs[\u001b[39m'\u001b[39m\u001b[39mdevice\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice\n\u001b[0;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.9/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(epochs):\n",
    "    \n",
    "    data_en, data_es = load_data_nmt(word_to_idx_en, word_to_idx_es, file_en, file_es, lines_count=1, max_train=30)\n",
    "    data_en = torch.from_numpy(data_en)\n",
    "    data_es = torch.from_numpy(data_es)\n",
    "    data_en = data_en.to(device)\n",
    "    data_es = data_es.to(device)\n",
    "\n",
    "    loss = nmt(data_en, data_es)\n",
    "\n",
    "    print(loss)\n",
    "\n",
    "    for param in parameters:\n",
    "        param.retain_grad()\n",
    "\n",
    "    optimizer.zero_grad() \n",
    "    loss.backward(retain_graph=True)\n",
    "    torch.nn.utils.clip_grad_norm_(parameters, 1.0)\n",
    "    # for param in parameters:\n",
    "        # print(param.grad)\n",
    "    optimizer.step()\n",
    "\n",
    "    # if i%100 == 0:\n",
    "    #     torch.save({\n",
    "    #         \"params_enc\" : nmt.encoder.RNN.params,\n",
    "    #         \"params_dec\" : nmt.decoder.RNN.params,\n",
    "    #         \"mid_layer\" : (nmt.mid_layer.weight, nmt.mid_layer.bias),\n",
    "    #         \"optime\" : optimizer.state_dict()\n",
    "    #     }, \"check.pt\")\n",
    "\n",
    "# torch.save({\n",
    "#     \"params_enc\" : nmt.encoder.RNN.params,\n",
    "#     \"params_dec\" : nmt.decoder.RNN.params,\n",
    "#     \"mid_layer\" : (nmt.mid_layer.weight, nmt.mid_layer.bias),\n",
    "#     \"optime\" : optimizer.state_dict()\n",
    "# }, \"check.pt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "el sr savary por tanto no se han alcanzado la necesidad de que la union europea se ha presentado un aspecto del sr gusinsky <END> <END> lo que se refiere a la comision <END> que la aprobacion de la comision se plantean en que la comision se ha desarrollado a los compromisos de que se pueda garantizar lo que se ******\n"
     ]
    }
   ],
   "source": [
    "str = \"<START> \"\n",
    "start_string = \"here improvements should be made\"\n",
    "start_string = str + start_string + \" <END>\"\n",
    "\n",
    "words = start_string.split()\n",
    "num_len_start = len(words)\n",
    "word_enc = [word_to_idx_en[word] for word in words]\n",
    "word_enc = torch.tensor(word_enc).unsqueeze(0)\n",
    "\n",
    "hidden_enc_states, prev_h, mask = nmt.encoder(word_enc) # (1, hidden_state_dim)\n",
    "\n",
    "prev_c = torch.zeros((1, prev_h.shape[1]))\n",
    "prev_h = nmt.\n",
    "\n",
    "\n",
    "start_weights = nmt.decoder.RNN.params['W_embed'][nmt.decoder.RNN.start_token]\n",
    "start_weights = torch.unsqueeze(start_weights, 0) # (1, word_vec_dim)\n",
    "\n",
    "curr_x = start_weights\n",
    "\n",
    "next_h, next_c = None, None\n",
    "max_length = 60\n",
    "\n",
    "rnn = nmt.decoder.RNN\n",
    "captions = rnn.null_token * torch.ones((1, max_length), dtype=torch.int32)\n",
    "\n",
    "letter_or_word = words\n",
    "\n",
    "for i in range(max_length):\n",
    "    context = attention_context_vector(prev_h, hidden_enc_states, rnn.params[\"W_attention1\"],\\\n",
    "                                rnn.params[\"b_attention1\"], rnn.params[\"W_attention2\"], mask)\n",
    "    if(architecture == \"rnn\"):\n",
    "        next_h = rnn_attention_step_forward(curr_x, prev_h, rnn.params[\"Wx\"], rnn.params[\"Wh\"],\\\n",
    "                                            rnn.params[\"b\"], context, rnn.params[\"W_context\"])\n",
    "    else:\n",
    "        next_h, next_c = lstm_attention_step_forward(curr_x, prev_h, prev_c, rnn.params[\"Wx\"], rnn.params[\"Wh\"],\\\n",
    "                                                    rnn.params[\"b\"], context, rnn.params[\"W_context\"])\n",
    "\n",
    "    out = affine_forward(next_h, rnn.params[\"W_vocab\"], rnn.params[\"b_vocab\"])\n",
    "    T = 0.5\n",
    "    out = torch.exp(out/T)\n",
    "    out = out / torch.sum(out, dim = 1)\n",
    "    indices = torch.multinomial(out, 1).squeeze(0)\n",
    "    captions[0, i] = indices\n",
    "    prev_h = next_h\n",
    "    prev_c = next_c\n",
    "    curr_x = rnn.params[\"W_embed\"][indices]\n",
    "\n",
    "captions = captions.tolist()\n",
    "\n",
    "file = open(\"out.txt\",'w')\n",
    "\n",
    "for i in range(len(captions)):\n",
    "    words = [reverse_dict_es[val] for val in captions[i]]\n",
    "    for word in words:\n",
    "        file.write(word + \" \")\n",
    "        print(word + \" \", end = \"\")\n",
    "    print(\"******\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
